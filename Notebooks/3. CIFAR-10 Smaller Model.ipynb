{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "B = tf.keras.backend\n",
    "\n",
    "#Set seeds for reproducability\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "seed = 6\n",
    "import random as rn\n",
    "rn.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + B.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "(x_train_init, y_train_init), (x_test_init, y_test_init) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep\n",
    "\n",
    "mean = np.mean(x_train_init,axis=(0,1,2,3))\n",
    "std = np.std(x_train_init,axis=(0,1,2,3))\n",
    "x_train = (x_train_init-mean)/(std)\n",
    "x_test = (x_test_init-mean)/(std)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train_init,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test_init,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 595,498\n",
      "Trainable params: 595,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),activation=gelu,input_shape=x_train.shape[1:], padding='same'))\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),activation=gelu, padding='same'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),activation=gelu, padding='same'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),activation=gelu, padding='same'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation=gelu))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(units=10,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(lr=0.003),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch):\n",
    "    lrate = init_lr\n",
    "    if epoch > 9:\n",
    "        lrate = 0.0025\n",
    "    if epoch > 19:\n",
    "        lrate = 0.0015\n",
    "    if epoch > 29:\n",
    "        lrate = 0.0075\n",
    "    if epoch > 44:\n",
    "        lrate = 0.0035\n",
    "    return lrate\n",
    "\n",
    "def lr_decay(epoch):\n",
    "    return init_lr * decay ** epoch\n",
    "\n",
    "class LrHistory(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        print(\"Learning rate:\", K.get_value(model.optimizer.lr))\n",
    "\n",
    "        \n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, restore_best_weights=True),\n",
    "# checkpoint = ModelCheckpoint(\"best_model_checkpoint.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max'),\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0035\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3895 - accuracy: 0.5100 - val_loss: 1.0523 - val_accuracy: 0.6317 - lr: 0.0035\n",
      "Learning rate: 0.00315\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.1873 - accuracy: 0.5928 - val_loss: 0.9987 - val_accuracy: 0.6577 - lr: 0.0032\n",
      "Learning rate: 0.002835\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.0900 - accuracy: 0.6269 - val_loss: 0.8998 - val_accuracy: 0.6945 - lr: 0.0028\n",
      "Learning rate: 0.0025515\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0269 - accuracy: 0.6506 - val_loss: 0.8836 - val_accuracy: 0.6921 - lr: 0.0026\n",
      "Learning rate: 0.00229635\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.9533 - accuracy: 0.6763 - val_loss: 0.7994 - val_accuracy: 0.7257 - lr: 0.0023\n",
      "Learning rate: 0.002066715\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 0.8906 - accuracy: 0.6935 - val_loss: 0.7901 - val_accuracy: 0.7303 - lr: 0.0021\n",
      "Learning rate: 0.0018600435\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.8261 - accuracy: 0.7169 - val_loss: 0.8196 - val_accuracy: 0.7255 - lr: 0.0019\n",
      "Learning rate: 0.0016740392\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.7732 - accuracy: 0.7365 - val_loss: 0.7438 - val_accuracy: 0.7476 - lr: 0.0017\n",
      "Learning rate: 0.0015066352\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.7218 - accuracy: 0.7530 - val_loss: 0.7220 - val_accuracy: 0.7636 - lr: 0.0015\n",
      "Learning rate: 0.0013559717\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.6835 - accuracy: 0.7662 - val_loss: 0.6810 - val_accuracy: 0.7711 - lr: 0.0014\n",
      "Learning rate: 0.0012203745\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.6346 - accuracy: 0.7797 - val_loss: 0.6622 - val_accuracy: 0.7775 - lr: 0.0012\n",
      "Learning rate: 0.0010983371\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 0.5835 - accuracy: 0.7995 - val_loss: 0.6527 - val_accuracy: 0.7849 - lr: 0.0011\n",
      "Learning rate: 0.0009885033\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.5597 - accuracy: 0.8069 - val_loss: 0.6525 - val_accuracy: 0.7829 - lr: 9.8850e-04\n",
      "Learning rate: 0.00088965305\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.5211 - accuracy: 0.8187 - val_loss: 0.6443 - val_accuracy: 0.7897 - lr: 8.8965e-04\n",
      "Learning rate: 0.00080068776\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.4924 - accuracy: 0.8268 - val_loss: 0.6129 - val_accuracy: 0.7961 - lr: 8.0069e-04\n",
      "Learning rate: 0.000720619\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4669 - accuracy: 0.8355 - val_loss: 0.6205 - val_accuracy: 0.7938 - lr: 7.2062e-04\n",
      "Learning rate: 0.0006485571\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.4362 - accuracy: 0.8468 - val_loss: 0.6240 - val_accuracy: 0.7975 - lr: 6.4856e-04\n",
      "Learning rate: 0.00058370136\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4203 - accuracy: 0.8516 - val_loss: 0.6039 - val_accuracy: 0.8031 - lr: 5.8370e-04\n",
      "Learning rate: 0.00052533124\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 0.4036 - accuracy: 0.8590 - val_loss: 0.6105 - val_accuracy: 0.8002 - lr: 5.2533e-04\n",
      "Learning rate: 0.0004727981\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3819 - accuracy: 0.8669 - val_loss: 0.6223 - val_accuracy: 0.8057 - lr: 4.7280e-04\n",
      "Learning rate: 0.0004255183\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.3652 - accuracy: 0.8705 - val_loss: 0.5965 - val_accuracy: 0.8097 - lr: 4.2552e-04\n",
      "Learning rate: 0.00038296645\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3450 - accuracy: 0.8766 - val_loss: 0.6094 - val_accuracy: 0.8092 - lr: 3.8297e-04\n",
      "Learning rate: 0.00034466982\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.3370 - accuracy: 0.8807 - val_loss: 0.6206 - val_accuracy: 0.8082 - lr: 3.4467e-04\n",
      "Learning rate: 0.00031020283\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3295 - accuracy: 0.8810 - val_loss: 0.6061 - val_accuracy: 0.8139 - lr: 3.1020e-04\n",
      "Learning rate: 0.00027918254\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3216 - accuracy: 0.8863 - val_loss: 0.6023 - val_accuracy: 0.8125 - lr: 2.7918e-04\n",
      "Learning rate: 0.00025126428\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3094 - accuracy: 0.8910 - val_loss: 0.6027 - val_accuracy: 0.8147 - lr: 2.5126e-04\n"
     ]
    }
   ],
   "source": [
    "init_lr = 0.0035\n",
    "decay = 0.9\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "               keras.callbacks.LearningRateScheduler(lr_decay),\n",
    "               LrHistory(),\n",
    "               early_stop],\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[856   7  21  17  11   1   5  10  32  40]\n",
      " [ 13 893   2   3   1   0   6   1   8  73]\n",
      " [ 52   2 709  53  61  37  53  23   4   6]\n",
      " [ 21   3  49 621  50 142  64  26  10  14]\n",
      " [ 18   2  39  46 788  18  38  41   7   3]\n",
      " [  9   1  40 145  33 709  22  36   3   2]\n",
      " [  5   3  28  33  19  13 891   5   2   1]\n",
      " [ 13   2  18  45  30  40   4 844   1   3]\n",
      " [ 54  18   8   9   4   4   2   3 880  18]\n",
      " [ 20  38   4   9   3   0   3   2  15 906]]\n",
      "[[0.856 0.007 0.021 0.017 0.011 0.001 0.005 0.01  0.032 0.04 ]\n",
      " [0.013 0.893 0.002 0.003 0.001 0.    0.006 0.001 0.008 0.073]\n",
      " [0.052 0.002 0.709 0.053 0.061 0.037 0.053 0.023 0.004 0.006]\n",
      " [0.021 0.003 0.049 0.621 0.05  0.142 0.064 0.026 0.01  0.014]\n",
      " [0.018 0.002 0.039 0.046 0.788 0.018 0.038 0.041 0.007 0.003]\n",
      " [0.009 0.001 0.04  0.145 0.033 0.709 0.022 0.036 0.003 0.002]\n",
      " [0.005 0.003 0.028 0.033 0.019 0.013 0.891 0.005 0.002 0.001]\n",
      " [0.013 0.002 0.018 0.045 0.03  0.04  0.004 0.844 0.001 0.003]\n",
      " [0.054 0.018 0.008 0.009 0.004 0.004 0.002 0.003 0.88  0.018]\n",
      " [0.02  0.038 0.004 0.009 0.003 0.    0.003 0.002 0.015 0.906]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "Y_pred = model.predict(x_test)\n",
    "y_test_non_category = [ np.argmax(t) for t in y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in Y_pred ]\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "print(cnf_matrix)\n",
    "cm = cnf_matrix\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 43s 4ms/step - loss: 0.5964 - accuracy: 0.8097\n",
      "\n",
      "Test result: 80.970 loss: 0.596\n"
     ]
    }
   ],
   "source": [
    "# Test Model\n",
    "scores = model.evaluate(x_test, y_test, batch_size=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
